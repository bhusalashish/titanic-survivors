import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

#Load the data
titanic = sns.load_dataset('titanic.csv')
titanic.head(10)
titanic.describe()

#Visualize the count of number of survivors
sns.countplot(titanic['survived'],label="Count")

# Drop the columns
titanic = titanic.drop(['deck', 'embark_town', 'alive', 'class', 'alone', 'adult_male', 'who'], axis=1)

#Remove the rows with missing values
titanic = titanic.dropna(subset =['embarked', 'age'])
titanic.shape

#Encoding categorical data values (Transforming object data types to integers)
from sklearn.preprocessing import LabelEncoder
labelencoder = LabelEncoder()

#Encode sex column
titanic.iloc[:,2]= labelencoder.fit_transform(titanic.iloc[:,2].values)


#Encode embarked
titanic.iloc[:,7]= labelencoder.fit_transform(titanic.iloc[:,7].values)

#Split the data into independent 'X' and dependent 'Y' variables
X = titanic.iloc[:, 1:8].values 
Y = titanic.iloc[:, 0].values 

# Split the dataset into 80% Training set and 20% Testing set
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)

#Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

#Create a function within many Machine Learning Models
def models(X_train,Y_train):
  
	#Using Logistic Regression Algorithm to the Training Set
	from sklearn.linear_model import LogisticRegression
	log = LogisticRegression(random_state = 0)
	log.fit(X_train, Y_train)

	#Using KNeighborsClassifier Method of neighbors class to use Nearest Neighbor algorithm
	from sklearn.neighbors import KNeighborsClassifier
	knn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
	knn.fit(X_train, Y_train)

	#Using SVC method of svm class to use Support Vector Machine Algorithm
	from sklearn.svm import SVC
	svc_lin = SVC(kernel = 'linear', random_state = 0)
	svc_lin.fit(X_train, Y_train)

	#Using SVC method of svm class to use Kernel SVM Algorithm
	from sklearn.svm import SVC
	svc_rbf = SVC(kernel = 'rbf', random_state = 0)
	svc_rbf.fit(X_train, Y_train)

	#Using GaussianNB method of naïve_bayes class to use Naïve Bayes Algorithm
	from sklearn.naive_bayes import GaussianNB
	gauss = GaussianNB()
	gauss.fit(X_train, Y_train)

	#Using DecisionTreeClassifier of tree class to use Decision Tree Algorithm
	from sklearn.tree import DecisionTreeClassifier
	tree = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
	tree.fit(X_train, Y_train)

	#Using RandomForestClassifier method of ensemble class to use Random Forest Classification algorithm
	from sklearn.ensemble import RandomForestClassifier
	forest = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)
	forest.fit(X_train, Y_train)

	#print model accuracy on the training data.
	print('[0]Logistic Regression Training Accuracy:', log.score(X_train, Y_train))
	print('[1]K Nearest Neighbor Training Accuracy:', knn.score(X_train, Y_train))
	print('[2]Support Vector Machine (Linear Classifier) Training Accuracy:', svc_lin.score(X_train, Y_train))
	print('[3]Support Vector Machine (RBF Classifier) Training Accuracy:', svc_rbf.score(X_train, Y_train))
	print('[4]Gaussian Naive Bayes Training Accuracy:', gauss.score(X_train, Y_train))
	print('[5]Decision Tree Classifier Training Accuracy:', tree.score(X_train, Y_train))
	print('[6]Random Forest Classifier Training Accuracy:', forest.score(X_train, Y_train))

	return log, knn, svc_lin, svc_rbf, gauss, tree, forest

model = models(X_train,Y_train)

from sklearn.metrics import confusion_matrix 
for i in range(len(model)):
   cm = confusion_matrix(Y_test, model[i].predict(X_test)) 
   #extracting TN, FP, FN, TP
   TN, FP, FN, TP = confusion_matrix(Y_test, model[i].predict(X_test)).ravel()
   print(cm)
   print('Model[{}] Testing Accuracy = "{} !"'.format(i,  (TP + TN) / (TP + TN + FN + FP)))
